# Sales Insight Assistant  

An AI-powered assistant for analyzing sales data, predicting customer churn, and answering business questions through retrieval-augmented generation (RAG).  

**Purpose**: This project is designed as a **learning playground** to practice building an end-to-end AI/ML system â€” from **data ingestion â†’ analytics â†’ ML model training â†’ RAG â†’ serving APIs/UI â†’ testing/CI/CD**. Itâ€™s not production-grade but demonstrates how different components fit together.

---

## âœ¨ Features
- **Data Ingestion**  
  - Load raw CSVs (Superstore sales, Telco churn) into a SQLite database.
- **Analytics KPIs**  
  - Compute metrics (total sales, average discount, total orders).  
  - Store KPIs back into the database.
- **Churn Prediction**  
  - Train an XGBoost model on the Telco churn dataset.  
  - Expose a `/predict` endpoint that returns churn probability for a customer.
- **Retrieval-Augmented Generation (RAG)**  
  - Index text/PDF docs into a FAISS vector store.  
  - Query with natural language and get context-aware answers.
- **API Service**  
  - FastAPI endpoints:  
    - `/kpis` â†’ current KPIs  
    - `/predict` â†’ churn prediction  
    - `/ask` â†’ RAG-powered Q&A  
- **UI**  
  - Streamlit-based frontend  

---

## ğŸ“‚ Project Structure
```

Sale Insight Assistant/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py              # FastAPI entrypoint
â”‚   â”œâ”€â”€ ingest.py           # Ingest raw data into SQLite
â”‚   â”œâ”€â”€ analytics.py        # Compute KPIs
â”‚   â”œâ”€â”€ train\_model.py      # Train churn model
â”‚   â”œâ”€â”€ rag\_index.py        # Build FAISS index
â”‚   â”œâ”€â”€ rag\_query.py        # Retrieval + generation pipeline
â”‚   â””â”€â”€ utils/              # Logging & exceptions
â”œâ”€â”€ tests/                  # Pytest-based tests
â”œâ”€â”€ data/                   # Raw data + generated DB/indexes (ignored in git)
â”œâ”€â”€ models/                 # Saved churn models (ignored in git)
â”œâ”€â”€ ui.py                   # Streamlit frontend
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile
â”œâ”€â”€ ci.yml                  # GitHub Actions CI workflow
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md               # This file

````

---

## âš™ï¸ Installation

Clone the repo:
```bash
git clone https://github.com/<your-username>/sale-insight-assistant.git
cd sale-insight-assistant
````

Create a virtual environment and install dependencies:

```bash
make setup
```

(or manually:)

```bash
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)
pip install -r requirements.txt
```

---

## ğŸ“Š Sample Datasets

To run this project, download the following datasets and place them under `data/raw/`:

* **Superstore Sales Dataset** (Kaggle):
  ğŸ‘‰ [Download here](https://www.kaggle.com/datasets/juhi1994/superstore)
  Save as `data/raw/superstore.csv`.

* **Telco Customer Churn Dataset** (IBM Sample):
  ğŸ‘‰ [Download here](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)
  Save as `data/raw/telco_churn.csv`.

---

## â–¶ï¸ Usage

### 1. Ingest Data

```bash
make ingest
```

### 2. Compute KPIs

```bash
make analytics
```

### 3. Train Churn Model

```bash
make train
```

### 4. Build RAG Index

Place business documents in `data/docs/` (TXT or PDF â†’ text). Then:

```bash
make rag-index
```

### 5. Run the API

```bash
make serve
```

Available endpoints:

* `GET /kpis`
* `POST /predict`
* `GET /ask?query=...`

Example churn prediction request:

```json
POST /predict
{
  "gender": "Female",
  "SeniorCitizen": 0,
  "Partner": "Yes",
  "Dependents": "No",
  "tenure": 12,
  "PhoneService": "Yes",
  "InternetService": "Fiber optic",
  "Contract": "Month-to-month",
  "PaymentMethod": "Electronic check",
  "MonthlyCharges": 70.35,
  "TotalCharges": 845.5
}
```

### 6. Optional UI

```bash
make ui
```

Launches a Streamlit dashboard.

---

## ğŸ§ª Running Tests

Run unit tests:

```bash
make test
```

Lint code:

```bash
make lint
```

---

## ğŸ¤– CI/CD

* **GitHub Actions (`ci.yml`)** runs on each push/PR to `main`:

  * Installs dependencies
  * Runs lint checks
  * Executes ingestion + analytics pipeline

---

## ğŸš€ Roadmap

* Add richer KPI analytics (monthly/quarterly trends).
* Improve churn model with hyperparameter tuning.
* Swap local generator (`distilgpt2`) with OpenAI.
* Containerize with Docker for deployment.

